# # -*- coding: utf-8 -*-
# """[ê³µê°œìš©, GPU] KcELECTRA-Base Fine Tune (NSMC) with PyTorch-Lightning v1.3.0 (last update 2021.07.22.)ì˜ ì‚¬ë³¸

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1A66xoOnr2vL1EJJe7C6myfQnTEdYIGnw

# Last Update @ 2021.07.22

# - Fix typo (training_epoch_endëŠ” ì•„ë¬´ê²ƒë„ ë°˜í™˜í•˜ì§€ ì•Šì•„ì•¼ í•¨)


# Update @ 2021.06.30

# - Fix typo (train_epoch_end -> training_epoch_end)

# Update @ 2021.05.12

# - PyTorch-Lightning v1.3.0 ê³µì‹  ë¦´ë¦¬ì¦ˆ 
# - Inference code (load ckpt) ì¶”ê°€  

# Update @ 2021.04.07

# - KcELECTRA ë° AutoModel ì¶”ê°€
# - ë¶ˆí•„ìš”í•œ warning ìœ ë°œ ì½”ë“œ ì œê±°(Logging)

# Update @ 2021.03.16. 

# - PyTorch-Lightning v1.3.0 ë°˜ì˜


# Update @ 2020.12.04

# - Huggingface Transformers 4.0.0  ë²„ì „ ë°˜ì˜

# # Package ì„¤ì¹˜ & ë°ì´í„° ë°›ê¸°

# ## PyTorch-Lightning & TransformersğŸ¤—

# PyTorchìì²´ ì™¸ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì€ ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# """

# import os

# """# íŒ¨í‚¤ì§€ import & ê¸°ë³¸ Args ì„¤ì •"""

import os
import pandas as pd

from pprint import pprint

import torch
from torch.utils.data import Dataset, DataLoader, TensorDataset
from torch.optim.lr_scheduler import ExponentialLR

from pytorch_lightning import LightningModule, Trainer, seed_everything

from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

import re
import emoji
from soynlp.normalizer import repeat_normalize

"""## ê¸°ë³¸ í•™ìŠµ Arguments"""

args = {
    'random_seed': 42, # Random Seed
    'pretrained_model': 'beomi/kcbert-large',  # Transformers PLM name
    'pretrained_tokenizer': '',  # Optional, Transformers Tokenizer Name. Overrides `pretrained_model`
    'batch_size': 4,
    'lr': 5e-6,  # Starting Learning Rate
    'epochs': 1,  # Max Epochs
    'max_length': 150,  # Max Length input size
    'train_data_path': "nsmc/ratings_train.txt",  # Train Dataset file 
    'val_data_path': "nsmc/ratings_test.txt",  # Validation Dataset file 
    'test_mode': False,  # Test Mode enables `fast_dev_run`
    'optimizer': 'AdamW',  # AdamW vs AdamP
    'lr_scheduler': 'exp',  # ExponentialLR vs CosineAnnealingWarmRestarts
    'fp16': True,  # Enable train on FP16(if GPU)
    'tpu_cores': 0,  # Enable TPU with 1 core or 8 cores
    'cpu_workers': os.cpu_count(),
}

# """# Model ë§Œë“¤ê¸° with Pytorch Lightning"""

class Model(LightningModule):
    def __init__(self, **kwargs):
        super().__init__()
        self.save_hyperparameters() # ì´ ë¶€ë¶„ì—ì„œ self.hparamsì— ìœ„ kwargsê°€ ì €ì¥ëœë‹¤.
        
        self.clsfier = AutoModelForSequenceClassification.from_pretrained(self.hparams.pretrained_model)
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.hparams.pretrained_tokenizer
            if self.hparams.pretrained_tokenizer
            else self.hparams.pretrained_model
        )

    def forward(self, **kwargs):
        return self.clsfier(**kwargs)

    def step(self, batch, batch_idx):
        data, labels = batch
        output = self(input_ids=data, labels=labels)

        # Transformers 4.0.0+
        loss = output.loss
        logits = output.logits

        preds = logits.argmax(dim=-1)

        y_true = list(labels.cpu().numpy())
        y_pred = list(preds.cpu().numpy())

        return {
            'loss': loss,
            'y_true': y_true,
            'y_pred': y_pred,
        }

    def training_step(self, batch, batch_idx):
        return self.step(batch, batch_idx)

    def validation_step(self, batch, batch_idx):
        return self.step(batch, batch_idx)

    def epoch_end(self, outputs, state='train'):
        loss = torch.tensor(0, dtype=torch.float)
        for i in outputs:
            loss += i['loss'].cpu().detach()
        loss = loss / len(outputs)

        y_true = []
        y_pred = []
        for i in outputs:
            y_true += i['y_true']
            y_pred += i['y_pred']
        
        acc = accuracy_score(y_true, y_pred)
        prec = precision_score(y_true, y_pred)
        rec = recall_score(y_true, y_pred)
        f1 = f1_score(y_true, y_pred)

        self.log(state+'_loss', float(loss), on_epoch=True, prog_bar=True)
        self.log(state+'_acc', acc, on_epoch=True, prog_bar=True)
        self.log(state+'_precision', prec, on_epoch=True, prog_bar=True)
        self.log(state+'_recall', rec, on_epoch=True, prog_bar=True)
        self.log(state+'_f1', f1, on_epoch=True, prog_bar=True)
        print(f'[Epoch {self.trainer.current_epoch} {state.upper()}] Loss: {loss}, Acc: {acc}, Prec: {prec}, Rec: {rec}, F1: {f1}')
        return {'loss': loss}
    
    def training_epoch_end(self, outputs):
        self.epoch_end(outputs, state='train')

    def validation_epoch_end(self, outputs):
        self.epoch_end(outputs, state='val')

    def configure_optimizers(self):
        if self.hparams.optimizer == 'AdamW':
            optimizer = AdamW(self.parameters(), lr=self.hparams.lr)
        elif self.hparams.optimizer == 'AdamP':
            from adamp import AdamP
            optimizer = AdamP(self.parameters(), lr=self.hparams.lr)
        else:
            raise NotImplementedError('Only AdamW and AdamP is Supported!')
        if self.hparams.lr_scheduler == 'cos':
            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)
        elif self.hparams.lr_scheduler == 'exp':
            scheduler = ExponentialLR(optimizer, gamma=0.5)
        else:
            raise NotImplementedError('Only cos and exp lr scheduler is Supported!')
        return {
            'optimizer': optimizer,
            'scheduler': scheduler,
        }

    def read_data(self, path):
        if path.endswith('xlsx'):
            return pd.read_excel(path)
        elif path.endswith('csv'):
            return pd.read_csv(path)
        elif path.endswith('tsv') or path.endswith('txt'):
            return pd.read_csv(path, sep='\t')
        else:
            raise NotImplementedError('Only Excel(xlsx)/Csv/Tsv(txt) are Supported')

    def clean(self, x):
        emojis = ''.join(emoji.UNICODE_EMOJI.keys())
        pattern = re.compile(f'[^ .,?!/@$%~ï¼…Â·âˆ¼()\x00-\x7Fã„±-í£{emojis}]+')
        url_pattern = re.compile(
            r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()@:%_\+.~#?&//=]*)')
        x = pattern.sub(' ', x)
        x = url_pattern.sub('', x)
        x = x.strip()
        x = repeat_normalize(x, num_repeats=2)
        return x

    def encode(self, x, **kwargs):
        return self.tokenizer.encode(
            self.clean(str(x)),
            padding='max_length',
            max_length=self.hparams.max_length,
            truncation=True,
            **kwargs,
        )

    def preprocess_dataframe(self, df):
        df['document'] = df['document'].map(self.encode)
        return df

    def dataloader(self, path, shuffle=False):
        df = self.read_data(path)
        df = self.preprocess_dataframe(df)

        dataset = TensorDataset(
            torch.tensor(df['document'].to_list(), dtype=torch.long),
            torch.tensor(df['label'].to_list(), dtype=torch.long),
        )
        return DataLoader(
            dataset,
            batch_size=self.hparams.batch_size * 1 if not self.hparams.tpu_cores else self.hparams.tpu_cores,
            shuffle=shuffle,
            num_workers=self.hparams.cpu_workers,
        )

    def train_dataloader(self):
        return self.dataloader(self.hparams.train_data_path, shuffle=True)

    def val_dataloader(self):
        return self.dataloader(self.hparams.val_data_path, shuffle=False)

# from pytorch_lightning.callbacks import ModelCheckpoint

# checkpoint_callback = ModelCheckpoint(
#     filename='epoch{epoch}-val_acc{val_acc:.4f}',
#     monitor='val_acc',
#     save_top_k=3,
#     mode='max',
#     auto_insert_metric_name=False,
# )

# """# í•™ìŠµ!

# > ğŸ’¡**NOTE**ğŸ’¡ 1epochë³„ë¡œ GPU P100ê¸°ì¤€ ì•½50ë¶„, GPU V100ê¸°ì¤€ ~15ë¶„ì´ ê±¸ë¦½ë‹ˆë‹¤. <br>
# > í•™ìŠµì‹œ ì•½ 0.92 ì´í•˜ì˜ validation accë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# > Update @ 2020.09.01
# > ìµœê·¼ Colab Proì—ì„œ V100ì´ ë°°ì •ë©ë‹ˆë‹¤.

# ```python
# # 1epoch
# loss=0.207, v_num=0, val_loss=0.221, val_acc=0.913, val_precision=0.914, val_recall=0.913, val_f1=0.914
# # 2epoch
# loss=0.152, v_num=0, val_loss=0.213, val_acc=0.918, val_precision=0.912, val_recall=0.926, val_f1=0.919
# # 3epoch
# loss=0.135, v_num=0, val_loss=0.225, val_acc=0.919, val_precision=0.907, val_recall=0.936, val_f1=0.921
# ```
# """

# print("Using PyTorch Ver", torch.__version__)
# print("Fix Seed:", args['random_seed'])
# seed_everything(args['random_seed'])
# model = Model(**args)

# print(":: Start Training ::")
# trainer = Trainer(
#     callbacks=[checkpoint_callback],
#     max_epochs=args['epochs'],
#     fast_dev_run=args['test_mode'],
#     num_sanity_val_steps=None if args['test_mode'] else 0,
#     # For GPU Setup
#     deterministic=torch.cuda.is_available(),
#     gpus=[0] if torch.cuda.is_available() else None,  # 0ë²ˆ idx GPU  ì‚¬ìš©
#     precision=16 if args['fp16'] and torch.cuda.is_available() else 32,
#     # For TPU Setup
#     # tpu_cores=args['tpu_cores'] if args['tpu_cores'] else None,
# )
# trainer.fit(model)

"""# Inference"""

# from glob import glob

# latest_ckpt = sorted(glob('./lightning_logs/version_1/checkpoints/*.ckpt'))[0]

# model = Model.load_from_checkpoint(latest_ckpt)

# def infer(x):
#     return torch.softmax(
#         model(**model.tokenizer(x, return_tensors='pt')
#     ).logits, dim=-1)

# print(infer('ë…¸ì¼ '))

# print(infer('ì´  ì˜í™”  ê¿€ì¼! ì™„ì¡´  ì¶”ì²œìš”  '))